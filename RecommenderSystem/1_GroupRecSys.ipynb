{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import implicit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from cdhf.data import Data\n",
    "from implicit import evaluation\n",
    "from tqdm.notebook import tqdm as log_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "random.seed(42)\n",
    "random_sample = random.sample(range(10, 300), 1)\n",
    "data = Data(\"../input/mmdata.json\")\n",
    "data.load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records([vars(cm) for cm in data.channel_members])\n",
    "df[\"index\"] = df[\"channel_id\"] + \"-\" + df[\"user_id\"]\n",
    "df.set_index('index', inplace=True)\n",
    "\n",
    "df_grouped_users = df.groupby([\"channel_id\"]).count()\n",
    "allowed_channels = df_grouped_users[df_grouped_users[\"user_id\"] > 5].index.array\n",
    "df = df[df[\"channel_id\"].isin(allowed_channels)]\n",
    "df['u_id'] = df['user_id'].astype(\"category\").cat.codes\n",
    "df['c_id'] = df['channel_id'].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeasureOfConfidence():\n",
    "    def calculate(self):\n",
    "        pass\n",
    "\n",
    "    def name(self):\n",
    "        pass\n",
    "\n",
    "    def preprocess_cui(self, cui, df):\n",
    "        pass\n",
    "\n",
    "class BinaryMeasureOfConfidence(MeasureOfConfidence):\n",
    "    def calculate(self, r_ui, alpha):\n",
    "        return 1\n",
    "\n",
    "    def name(self):\n",
    "        return \"Binary Measure < 1 >\"\n",
    "\n",
    "class SimpleMeasureOfConfidence(MeasureOfConfidence):\n",
    "    def calculate(self, r_ui, alpha):\n",
    "        return 1 + alpha*r_ui\n",
    "\n",
    "    def name(self):\n",
    "        return \"Simple Measure < 1 + alpha*r_ui >\"\n",
    "\n",
    "class LogMeasureOfConfidence(MeasureOfConfidence):\n",
    "    def calculate(self, r_ui, alpha):\n",
    "        return 1 + alpha*(math.log(1+(r_ui/0.0001)))\n",
    "\n",
    "    def name(self):\n",
    "        return \"Log Measure < 1 + alpha*log(1 + r_ui/epsilon) >\"\n",
    "\n",
    "class EvalResults():\n",
    "\n",
    "    def __init__(self, name, confidence, metrics, factors, alpha, iterations):\n",
    "        self.name = name\n",
    "        self.confidence = confidence\n",
    "        self.metrics = metrics\n",
    "        self.factors = factors\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePowerFunction():\n",
    "    def name():\n",
    "        pass\n",
    "\n",
    "    def calculate():\n",
    "        pass\n",
    "\n",
    "\n",
    "class SimpleLScore(BasePowerFunction):\n",
    "    def name(self):\n",
    "        return \"Simple Score MSG Count\"\n",
    "\n",
    "    def calculate(self, s_l, s_r):\n",
    "        return s_l\n",
    "\n",
    "class SimpleRScore(BasePowerFunction):\n",
    "    def name(self):\n",
    "        return \"Simple Score Feature\"\n",
    "\n",
    "    def calculate(self, s_l, s_r):\n",
    "        return s_r\n",
    "\n",
    "class PowerFuncScore(BasePowerFunction):\n",
    "    def name(self):\n",
    "        return \"MSG COUNT To The Power of Score Feature\"\n",
    "\n",
    "    def calculate(self, s_l, s_r):\n",
    "        return s_l.pow(1 + s_r)\n",
    "\n",
    "\n",
    "\n",
    "power_functions = [SimpleLScore(), SimpleRScore(), PowerFuncScore()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculateSparsity(df):\n",
    "    users = list(np.sort(df.u_id.unique()))\n",
    "    channels = list(np.sort(df.c_id.unique()))\n",
    "    interactions = list(df.score)\n",
    "\n",
    "    sparsity = (1 - (len(interactions)/(len(users)*len(channels))))*100\n",
    "\n",
    "    user_sparsity_df = df.copy()\n",
    "    user_sparsity_df = user_sparsity_df.groupby([\"u_id\"]).count()\n",
    "    user_sparsity_max = user_sparsity_df.c_id.max()\n",
    "    user_sparsity_df['USS'] = (1 - (user_sparsity_df[\"c_id\"]/user_sparsity_max)) * 100\n",
    "\n",
    "    item_sparsity_df = df.copy()\n",
    "    item_sparsity_df = item_sparsity_df.groupby([\"c_id\"]).count()\n",
    "    item_rating_max = item_sparsity_df.u_id.max()\n",
    "    item_sparsity_df['ISS'] = (1 - (item_sparsity_df[\"u_id\"]/item_rating_max)) * 100\n",
    "\n",
    "    return sparsity, user_sparsity_df['USS'].mean(), item_sparsity_df['ISS'].mean(), len(users), len(channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execute(df):\n",
    "    sparsity, USS, ISS, users, channels = calculateSparsity(df)\n",
    "    confidence_measures = [SimpleMeasureOfConfidence(), BinaryMeasureOfConfidence(), LogMeasureOfConfidence()]\n",
    "    alpha_values = [40]\n",
    "    factor_options = [150]\n",
    "    iteration_opttions =[25] \n",
    "\n",
    "    evaluation_results = []\n",
    "\n",
    "    for alpha_val in alpha_values:\n",
    "        for confidence_measure in confidence_measures:\n",
    "            c_ui = df['score'].astype(float).copy()\n",
    "            c_ui.apply(confidence_measure.calculate, args= (alpha_val,))\n",
    "            \n",
    "            sparse_item_user = sparse.csr_matrix(( c_ui, (df['c_id'].astype(int), df['u_id'].astype(int))))\n",
    "            # sparse_user_item = sparse.csr_matrix(( c_ui, (df['u_id'], df['c_id'])))\n",
    "            for randomInt in random_sample:\n",
    "                data_train, data_test = evaluation.train_test_split(sparse_item_user, 0.25, randomInt)\n",
    "\n",
    "                for factors in factor_options:\n",
    "                    for iterations in iteration_opttions:\n",
    "                        regularization = 0.1                \n",
    "                        models = []        \n",
    "                        \n",
    "                        models.append(implicit.als.AlternatingLeastSquares(num_threads = 4,  factors=factors, regularization=regularization, iterations=iterations))    \n",
    "                        models.append(implicit.cpu.bpr.BayesianPersonalizedRanking(num_threads = 4, factors=factors, iterations=iterations))\n",
    "                        models.append(implicit.cpu.lmf.LogisticMatrixFactorization(num_threads = 4))                    \n",
    "                        models.append(implicit.nearest_neighbours.ItemItemRecommender(num_threads = 4, K=20))\n",
    "            \n",
    "                        for model in models:\n",
    "                            try:\n",
    "                                model.fit(data_train, show_progress=False)   \n",
    "                                ranking = implicit.evaluation.ranking_metrics_at_k(model, data_train, data_test, K=10, show_progress=False)\n",
    "                                evalResult = EvalResults(name = model.__class__, confidence = confidence_measure.name(), metrics = ranking, factors = factors, alpha = alpha_val, iterations = iterations )                    \n",
    "                                evaluation_results.append(evalResult)                                       \n",
    "                            except BaseException as e:\n",
    "                                print(f'Failed when processing {model} with {e}:{type(e)}')\n",
    "                                continue\n",
    "\n",
    "    df_evals = pd.DataFrame.from_records([[er.name.__doc__.split(\"\\n\\n\")[0], er.metrics[\"auc\"], er.metrics[\"precision\"], er.factors, er.iterations, er.alpha, er.confidence, sparsity, USS, ISS, users, channels , er.metrics[\"ndcg\"], er.metrics[\"map\"]] for er in evaluation_results], columns=[\"name\",'AUC', 'precision', 'factors', 'iterations', 'alpha', 'confidence_measure', \"sparsity\", \"USS\", \"ISS\", \"users\", \"channels\", 'ndcg', 'map'])\n",
    "    df_evals = df_evals.sort_values(['AUC', 'precision', 'iterations', 'factors'], ascending = [False, False, True, True])\n",
    "    \n",
    "    \n",
    "    return df_evals  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org_evals = pd.DataFrame()\n",
    "for org_unit in log_progress(data.org_unit_members):\n",
    "    org_unit_memebers = data.org_unit_members[org_unit]\n",
    "    org_df = df[df[\"user_id\"].isin(org_unit_memebers)]\n",
    "    if(org_df.empty):\n",
    "        continue\n",
    "    org_df[\"score\"] = org_df[\"msg_count\"].copy()\n",
    "    df_org_evals = pd.concat([execute(org_df[[\"user_id\", \"channel_id\", \"score\", \"u_id\", \"c_id\"]]) , df_org_evals])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org_evals.to_pickle(\"OrgUnitClustering.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams_evals = pd.DataFrame()\n",
    "for team in log_progress(data.teams):\n",
    "\n",
    "    team_members = [m.user_id for m in team.team_members]\n",
    "    team_df = df[df[\"user_id\"].isin(team_members)]\n",
    "    if(team_df.empty):\n",
    "        continue        \n",
    "    team_df[\"score\"] = team_df[\"msg_count\"]\n",
    "    df_teams_evals = pd.concat([execute(team_df[[\"user_id\", \"channel_id\", \"score\", \"u_id\", \"c_id\"]]) , df_teams_evals])\n",
    "\n",
    "df_teams_evals.to_pickle(\"TeamsClustering.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams_evals.to_pickle(\"TeamsClustering.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_filtering_cluster(user_ids, df):    \n",
    "    cluster_df = df[df[\"user_id\"].isin(user_ids)].copy()    \n",
    "    if(cluster_df.empty):\n",
    "        return        \n",
    "    cluster_df[\"score\"] = cluster_df[\"msg_count\"]\n",
    "    return execute(cluster_df[[\"user_id\", \"channel_id\", \"score\", \"u_id\", \"c_id\"]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_evals = {}\n",
    "\n",
    "clustering_path = \"Processed/Clustering\"\n",
    "cluser_plks = [f for f in os.listdir(clustering_path) if os.path.isfile(os.path.join(clustering_path, f))]\n",
    "cluser_plks = [cpks for cpks in cluser_plks if cpks.startswith('clusters-0.5') or cpks.startswith('clusters-1-')]\n",
    "\n",
    "for cluser_plk in log_progress(cluser_plks):\n",
    "    cluster_evals[cluser_plk] =  pd.DataFrame()\n",
    "    df_cluster  = pd.read_pickle(os.path.join(clustering_path, cluser_plk))   \n",
    "    result = df_cluster['nodes'].apply(collaborative_filtering_cluster, args=(df, ))    \n",
    "    for ix, val in result.items():    \n",
    "        if(val is None):\n",
    "            continue\n",
    "        cluster_evals[cluser_plk] = pd.concat([val , cluster_evals[cluser_plk]])            \n",
    "\n",
    "for key in cluster_evals:    \n",
    "    cluster_evals[key].to_pickle(f\"{key}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c6710c6338c3a43678f1ef28d4c6ed63a77d410a91cdedd60b3283f85d762de"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
