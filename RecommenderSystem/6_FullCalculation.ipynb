{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from cdhf.data import Data\n",
    "from tqdm.notebook import tqdm as log_progress\n",
    "from utils.helpers import execute, save_pckl, create_init_dataframe, collaborative_filtering_cluster\n",
    "from utils.power_functions import PowerFuncScore\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random_sample = random.sample(range(10, 300), 1)\n",
    "data = Data(\"../input/mmdata.json\")\n",
    "data.load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_init_dataframe(data).drop(columns=[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Cluster with Complex Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clustering_path = \"Processed/Clustering\"\n",
    "cluser_plks = [f for f in os.listdir(clustering_path) if os.path.isfile(os.path.join(clustering_path, f))]\n",
    "cluser_plks = [cpks for cpks in cluser_plks if cpks.startswith('clusters-0.5') or cpks.startswith('clusters-1-')]\n",
    "\n",
    "feature_path = \"Processed/UserChannel\"\n",
    "feature_files = [f for f in os.listdir(feature_path) if os.path.isfile(os.path.join(feature_path, f))]\n",
    "power_function = PowerFuncScore()\n",
    "\n",
    "feature_evals = {}\n",
    "for feature_file in log_progress(feature_files):                            \n",
    "    feature_df = pd.read_pickle(os.path.join(feature_path, feature_file)).rename(columns={\"userid\":\"user_id\", \"channelid\":\"channel_id\"})    \n",
    "    cluster_evals = {}\n",
    "    for cluser_plk in log_progress(cluser_plks):\n",
    "        df_cluster  = pd.read_pickle(os.path.join(clustering_path, cluser_plk))   \n",
    "        cluster_evals[cluser_plk] = pd.DataFrame()\n",
    "        for ix, row in log_progress(df_cluster.iterrows(), total=df_cluster.shape[0]):            \n",
    "            user_ids = row[\"nodes\"]\n",
    "            cluster_user_df = df[df[\"user_id\"].isin(user_ids)]            \n",
    "            rec_df = pd.merge(cluster_user_df, feature_df, on=['user_id','channel_id'], how=\"outer\").fillna(0)    \n",
    "            if(rec_df.empty):\n",
    "                continue\n",
    "            \n",
    "            rec_df[\"score\"] = power_function.calculate(rec_df[\"msg_count\"], rec_df[\"score\"])                                    \n",
    "            cluster_evals[cluser_plk] = pd.concat([execute(rec_df[[\"user_id\", \"channel_id\", \"score\", \"u_id\", \"c_id\"]], random_sample) , cluster_evals[cluser_plk]])         \n",
    "    feature_evals[feature_file] = cluster_evals    \n",
    "\n",
    "\n",
    "for f_key in feature_evals:         \n",
    "    for c_key in feature_evals[f_key]:\n",
    "        path = f\"Evaulation/Complex/F/{f_key}/{c_key}\"          \n",
    "        save_pckl(path, feature_evals[f_key][c_key])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for f_key in feature_evals:         \n",
    "    for c_key in feature_evals[f_key]:\n",
    "        path = f\"Evaulation/Complex/F/{f_key}/{c_key}\"          \n",
    "        save_pckl(path, feature_evals[f_key][c_key])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All User-Channels with Complex Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evals = pd.DataFrame()\n",
    "\n",
    "feature_evals = {}\n",
    "\n",
    "feature_path = \"Processed/UserChannel\"\n",
    "feature_files = [f for f in os.listdir(feature_path) if os.path.isfile(os.path.join(feature_path, f))]\n",
    "power_function = PowerFuncScore()\n",
    "\n",
    "for feature_file in log_progress(feature_files): \n",
    "    try:\n",
    "\n",
    "        feature_df = pd.read_pickle(os.path.join(feature_path, feature_file)).rename(columns={\"userid\":\"user_id\", \"channelid\":\"channel_id\"})    \n",
    "\n",
    "        rec_df = pd.merge(df, feature_df, on=['user_id','channel_id'], how=\"outer\").fillna(0)    \n",
    "        rec_df[\"score\"] = power_function.calculate(rec_df[\"msg_count\"], rec_df[\"score\"])\n",
    "        feature_evals[feature_file] = execute(rec_df[[\"user_id\", \"channel_id\", \"score\", \"u_id\", \"c_id\"]], random_sample)            \n",
    "        path = f\"Evaulation/Complex/P/{feature_file}\"  \n",
    "        save_pckl(path, feature_evals[feature_file]) \n",
    "    except:\n",
    "        display(f\"failed for {feature_file}\")\n",
    "\n",
    "for key in feature_evals:            \n",
    "    path = f\"Evaulation/Complex/P/{key}\"  \n",
    "    save_pckl(path, feature_evals[key])         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Generated Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_evals = {}\n",
    "\n",
    "clustering_path = \"Processed/Clustering\"\n",
    "cluser_plks = [f for f in os.listdir(clustering_path) if os.path.isfile(os.path.join(clustering_path, f))]\n",
    "cluser_plks = [cpks for cpks in cluser_plks if cpks.startswith('clusters-0.5') or cpks.startswith('clusters-1-')]\n",
    "\n",
    "for cluser_plk in log_progress(cluser_plks):\n",
    "    cluster_evals[cluser_plk] =  pd.DataFrame()\n",
    "    df_cluster  = pd.read_pickle(os.path.join(clustering_path, cluser_plk))   \n",
    "    result = df_cluster['nodes'].apply(collaborative_filtering_cluster, args=(df, random_sample, ))    \n",
    "    for ix, val in result.items():    \n",
    "        if(val is None):\n",
    "            continue\n",
    "        cluster_evals[cluser_plk] = pd.concat([val , cluster_evals[cluser_plk]])       \n",
    "\n",
    "for key in cluster_evals:            \n",
    "    path = f\"Evaulation/Clustering/{key}\"  \n",
    "    save_pckl(path, cluster_evals[key])  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c6710c6338c3a43678f1ef28d4c6ed63a77d410a91cdedd60b3283f85d762de"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
